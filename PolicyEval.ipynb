{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pprint\n",
    "import sys\n",
    "if \"../\" not in sys.path:\n",
    "  sys.path.append(\"../\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OBS CURRENTLY NOT IN USE\n",
    "\n",
    "# Minataur blocked paths\n",
    "\n",
    "Blocked_minataur = {\n",
    "    0: [x for x in range(0,6)],\n",
    "    1: [5,11,17,23,29],\n",
    "    2: [x for x in range(24,30)],\n",
    "    3: [0,6,12,18,24]\n",
    "}\n",
    "\n",
    "Blocked_agent = {\n",
    "    0: [16,17,25,26,27,28,0,1,2,3,4,5],\n",
    "    1: [1,7,13,9,15,27,5,11,17,23,29],\n",
    "    2: [10,11,19,20,21,22,24,25,26,27,28,29],\n",
    "    3: [2,8,14,10,16,28,0,6,12,18,24]\n",
    "}\n",
    "\n",
    "agent_moves = {}\n",
    "minataur_moves = {}\n",
    "\n",
    "grid = np.arange(30).reshape((5,6))\n",
    "it = np.nditer(grid, flags=['multi_index'])\n",
    "\n",
    "while not it.finished:\n",
    "    s = it.iterindex\n",
    "    x,y  = it.multi_index\n",
    "    \n",
    "    moves_agent = []\n",
    "    moves_minataur = []\n",
    "    for move in range(4):\n",
    "        \n",
    "        if not s in Blocked_agent[move]:\n",
    "            moves_agent.append(move)\n",
    "        \n",
    "        if not s in Blocked_minataur[move]:\n",
    "            moves_minataur.append(move)\n",
    "            \n",
    "    moves_agent.append(4)\n",
    "    \n",
    "    agent_moves[s] = moves_agent\n",
    "    minataur_moves[s] = moves_minataur\n",
    "    \n",
    "    it.iternext()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "from gym.envs.toy_text import discrete\n",
    "\n",
    "STAY = 0\n",
    "UP = 1\n",
    "RIGHT = 2\n",
    "DOWN = 3\n",
    "LEFT = 4\n",
    "\n",
    "\n",
    "class GridworldEnv(discrete.DiscreteEnv):\n",
    "    \"\"\"\n",
    "    You are an agent on an MxN grid and your goal is to reach the terminal\n",
    "    state at the bottom right corner. There is also an minatours that is moving randomly\n",
    "    unifromaly, if he catches you, you will be eated. \n",
    "\n",
    "    You can take actions in each direction (UP=0, RIGHT=1, DOWN=2, LEFT=3, STAY=4).\n",
    "    Actions going off the edge leave you in your current state with a reward -1.\n",
    "    Actions going into a wall, leave you in your current state with a reward -1. \n",
    "    This make the agent learn not to bump into walls or edges. \n",
    "    \n",
    "    You receive a reward of -1 at each step until you reach a terminal state.\n",
    "    Your recive a reward of -10 if the minatours catches you\n",
    "    \n",
    "    There are 30*30 number of states, one state represent the tuple (agent_state, minataur_state) in the enviroment\n",
    "    the full set of states descirbe all possible combination of theese states. \n",
    "    \n",
    "    \n",
    "    \n",
    "    # LEFT TO FIX:\n",
    "        Alter the proababilities and next states when the minataur is at an edge. Now prob=0.25 for any of the directions.\n",
    "        Diffrent discount factors? \n",
    "            More important to avoid the minataurs then getting closer to the goal so discount factor < 1 is necessary.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    metadata = {'render.modes': ['human', 'ansi']}\n",
    "\n",
    "    def __init__(self, shape=[5,6]):\n",
    "\n",
    "        self.shape = shape\n",
    "    \n",
    "        # Number of states\n",
    "        nS = 900\n",
    "        #Number of actions\n",
    "        nA = 5\n",
    "        \n",
    "        #Shape of the maze\n",
    "        MAX_Y = shape[0]\n",
    "        MAX_X = shape[1]\n",
    "    \n",
    "        #Translation Matrix, used to find the state index\n",
    "        state_index = np.arange(nS).reshape((30,30))\n",
    "        \n",
    "        # Minataur blocked paths. Minataur can't go outside edges\n",
    "        Edge_up = [x for x in range(0,6)]\n",
    "        Edge_right = [5,11,17,23,29]\n",
    "        Edge_down = [x for x in range(24,30)]\n",
    "        Edge_left = [0,6,12,18,24]\n",
    "        \n",
    "\n",
    "        #Agents blocked paths. Agent can't go outside edges or into walls.\n",
    "        Blocked_agent_up = [16,17,25,26,27,28,0,1,2,3,4,5]\n",
    "        Blocked_agent_right = [1,7,13,9,15,27,5,11,17,23,29]\n",
    "        Blocked_agent_down = [10,11,19,20,21,22,24,25,26,27,28,29]\n",
    "        Blocked_agent_left = [2,8,14,10,16,28,0,6,12,18,24]\n",
    "        \n",
    "        #Init of transition matrix\n",
    "        P = {}\n",
    "\n",
    "        #Matrix (30*30) with all numbered possible states\n",
    "        grid = np.arange(nS).reshape((30,30))\n",
    "        #Iteration tool, instead of using two for loops\n",
    "        it = np.nditer(grid, flags=['multi_index'])\n",
    "\n",
    "        while not it.finished:\n",
    "\n",
    "            s = it.iterindex\n",
    "            # x= agent current state (0-29) , y = minataur current state  (0-29)\n",
    "            x,y  = it.multi_index\n",
    "\n",
    "            #Stopping condition, when have reached terminal state (either end of maze or eaten)\n",
    "            is_done = lambda x: x == 28\n",
    "            is_eaten = lambda x,y: x == y\n",
    "            #Negative reward for everytime step we are not in the terminal state\n",
    "            reward = 0.0 if is_done(x) else -1.0\n",
    "            #Negative reward if out state and the minataurs is the same(we have been eaten)\n",
    "            if is_eaten(x,y) == True:\n",
    "                reward = -1\n",
    "\n",
    "                        \n",
    "            P[s] = {a : [[]] for a in range(nA)}\n",
    "\n",
    "            # We're stuck in a terminal state if the agent is in the terminal node, no matter where\n",
    "            # the minataur is\n",
    "            if is_done(x) or is_eaten(x,y):\n",
    "                P[s][UP] = [([1.0], [s], [reward], True)]\n",
    "                P[s][RIGHT] = [([1.0], [s], [reward], True)]\n",
    "                P[s][DOWN] = [([1.0], [s], [reward], True)]\n",
    "                P[s][LEFT] = [([1.0], [s], [reward], True)]\n",
    "                P[s][STAY] = [([1.0], [s], [reward], True)]\n",
    "            # Not a terminal state\n",
    "            else:\n",
    "                # First determine states depending on out choice of action\n",
    "                #If we go up when we are at the top of maze, we end up the same pos.\n",
    "                #If we go up when we are at any position in Walls_Up, we end up in same pos\n",
    "                ns_up = x if x in Blocked_agent_up else x - MAX_X\n",
    "                ns_right = x if x in Blocked_agent_right else x + 1\n",
    "                ns_down = x if x in Blocked_agent_down else x + MAX_X\n",
    "                ns_left = x if x in Blocked_agent_left else x - 1\n",
    "\n",
    "                # Determine next state of the minataurs, 4 possibilies\n",
    "                ns_m_up = y if y in Edge_up else y - MAX_X\n",
    "                ns_m_right = y if y in Edge_right else y + 1\n",
    "                ns_m_down = y if y in Edge_down else y + MAX_X\n",
    "                ns_m_left = y if y in Edge_left else y -1\n",
    "\n",
    "                # After an action a, we can be in four possible states. A total of 16 \n",
    "            \n",
    "                agent_next_states = [x,ns_up, ns_right, ns_down, ns_left]\n",
    "                minataur_next_states = [y,ns_m_up, ns_m_right,ns_m_down, ns_m_left]\n",
    "     \n",
    "                action = 0\n",
    "                \n",
    "                # Buliding transition matrix P\n",
    "                for ns in agent_next_states:\n",
    "                    prob_list = []\n",
    "                    next_state_list = []\n",
    "                    reward_list = []\n",
    "\n",
    "                    for ns_m in minataur_next_states:\n",
    "                        reward = -1\n",
    "                        ns_index = state_index[ns][ns_m]\n",
    "                        next_state_list.append(ns_index)\n",
    "                        prob_list.append(0.25)\n",
    "                        \n",
    "                        # If the agent next state == minataurs next state -> NO GOOD! \n",
    "                        if ns == ns_m:\n",
    "                            reward=-10\n",
    "                        # If the agent next state == minataurs current state -> EVEN WORSE!\n",
    "                        if ns ==y:\n",
    "                            reward = -50\n",
    "\n",
    "                        reward_list.append(reward)\n",
    "                    \n",
    "                    \n",
    "                    P[s][action] = [(prob_list,next_state_list, reward_list, is_done(ns))] \n",
    "                    action += 1\n",
    "                    \n",
    "\n",
    "            it.iternext()\n",
    "\n",
    "        # Initial state distribution is uniform\n",
    "        isd = np.ones(nS) / nS\n",
    "\n",
    "        self.P = P\n",
    "\n",
    "        super(GridworldEnv, self).__init__(nS, nA, P, isd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = pprint.PrettyPrinter(indent=2)\n",
    "env = GridworldEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, theta=0.0001, discount_factor=0.8):\n",
    "    \"\"\"\n",
    "    Value Iteration Algorithm.\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI env. env.P represents the transition probabilities of the environment.\n",
    "            env.P[s][a] is a list of transition tuples (prob, next_state, reward, done).\n",
    "            env.nS is a number of states in the environment. \n",
    "            env.nA is a number of actions in the environment.\n",
    "        theta: We stop evaluation once our value function change is less than theta for all states.\n",
    "        discount_factor: Gamma discount factor.\n",
    "        \n",
    "    Returns:\n",
    "        A tuple (policy, V) of the optimal policy and the optimal value function.\n",
    "    \"\"\"\n",
    "    \n",
    "    def one_step_lookahead(state, V):\n",
    "        \"\"\"\n",
    "        Helper function to calculate the value for all action in a given state.\n",
    "        \n",
    "        Args:\n",
    "            state: The state to consider (int)\n",
    "            V: The value to use as an estimator, Vector of length env.nS\n",
    "        \n",
    "        Returns:\n",
    "            A vector of length env.nA containing the expected value of each action.\n",
    "        \"\"\"\n",
    "        A = np.zeros(env.nA)\n",
    "        for a in range(env.nA):\n",
    "            for prob, next_state, reward, done in env.P[state][a]:\n",
    "                #Prob a vector for the probabilities of the nex_state\n",
    "                #Prob a vector for the possible next states\n",
    "                # Rewards in thoose possible next states\n",
    "               \n",
    "                for i in range(len(prob)):\n",
    "                    A[a] += prob[i] * (reward[i] + discount_factor * V[next_state[i]])\n",
    "        \n",
    "               \n",
    "        return A\n",
    "    \n",
    "    V = np.zeros(env.nS)\n",
    "    maxIter = 50\n",
    "    i = 0\n",
    "    while True:\n",
    "        # Stopping condition\n",
    "        delta = 0\n",
    "        # Update each state...\n",
    "        for s in range(env.nS):\n",
    "            \n",
    "            # Do a one-step lookahead to find the best action\n",
    "            A = one_step_lookahead(s, V)\n",
    "            best_action_value = np.max(A)\n",
    "            # Calculate delta across all states seen so far\n",
    "            delta = max(delta, np.abs(best_action_value - V[s]))\n",
    "            # Update the value function. Ref: Sutton book eq. 4.10. \n",
    "            V[s] = best_action_value        \n",
    "        # Check if we can stop \n",
    "        \n",
    "        i +=1\n",
    "        print(\"Delta-Value \", delta)\n",
    "        if delta < theta:\n",
    "            break\n",
    "        if i >=maxIter:\n",
    "            break\n",
    "        \n",
    "    \n",
    "    # Create a deterministic policy using the optimal value function\n",
    "    policy = np.zeros([env.nS, env.nA])\n",
    "    for s in range(env.nS):\n",
    "        # One step lookahead to find the best action for this state\n",
    "        A = one_step_lookahead(s, V)\n",
    "        best_action = np.argmax(A)\n",
    "        # Always take the best action\n",
    "        policy[s, best_action] = 1.0\n",
    "    \n",
    "    return policy, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delta-Value  4.299200000000001\n",
      "Delta-Value  2.91747584\n",
      "Delta-Value  2.3923079987199998\n",
      "Delta-Value  2.190104300773376\n",
      "Delta-Value  2.121804144611625\n",
      "Delta-Value  2.089951825370763\n",
      "Delta-Value  2.0539359294969817\n",
      "Delta-Value  1.971819606084395\n",
      "Delta-Value  1.8888189019434236\n",
      "Delta-Value  1.6205700075560152\n",
      "Delta-Value  1.2686346116841385\n",
      "Delta-Value  0.9577824588032922\n",
      "Delta-Value  0.6878170709746634\n",
      "Delta-Value  0.5126607135741459\n",
      "Delta-Value  0.39171715347778147\n",
      "Delta-Value  0.2868499054496709\n",
      "Delta-Value  0.22254981307650112\n",
      "Delta-Value  0.1743024702046796\n",
      "Delta-Value  0.13715188406906975\n",
      "Delta-Value  0.10818202742025562\n",
      "Delta-Value  0.08544496445328775\n",
      "Delta-Value  0.0675383927941624\n",
      "Delta-Value  0.05340956017128562\n",
      "Delta-Value  0.04190321535110186\n",
      "Delta-Value  0.03170743463550707\n",
      "Delta-Value  0.02374664969412521\n",
      "Delta-Value  0.01802471107453485\n",
      "Delta-Value  0.013748745862312006\n",
      "Delta-Value  0.010526831799939629\n",
      "Delta-Value  0.008086191978041057\n",
      "Delta-Value  0.0062295053570338155\n",
      "Delta-Value  0.004811927928955129\n",
      "Delta-Value  0.003726137104258953\n",
      "Delta-Value  0.0028920523640501017\n",
      "Delta-Value  0.0022495906056754222\n",
      "Delta-Value  0.0017534700311827578\n",
      "Delta-Value  0.0013694339238163877\n",
      "Delta-Value  0.0010714791302852689\n",
      "Delta-Value  0.0008398068390675917\n",
      "Delta-Value  0.0006592988018141455\n",
      "Delta-Value  0.0005183791373966073\n",
      "Delta-Value  0.00040816092707274265\n",
      "Delta-Value  0.0003218042094843554\n",
      "Delta-Value  0.00025403153232517184\n",
      "Delta-Value  0.00020076133670343665\n",
      "Delta-Value  0.00015882973885439355\n",
      "Delta-Value  0.000125778819583644\n",
      "Delta-Value  9.969509146401379e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[3, 3, 2, 3, 4, 4],\n",
       "       [3, 4, 2, 3, 1, 1],\n",
       "       [3, 0, 2, 3, 2, 3],\n",
       "       [2, 2, 2, 2, 2, 3],\n",
       "       [1, 4, 4, 4, 0, 4]], dtype=int64)"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "policy, v = value_iteration(env)\n",
    "\n",
    "#print(\"Policy Probability Distribution:\")\n",
    "#print(policy)\n",
    "#print(\"\")\n",
    "\n",
    "#print(\"Reshaped Grid Policy (0=up, 1=right, 2=down, 3=left):\")\n",
    "#print(np.reshape(np.argmax(policy, axis=1), (30,30)))\n",
    "#print(\"\")\n",
    "\n",
    "#print(\"Value Function:\")\n",
    "#print(v)\n",
    "#print(\"\")\n",
    "\n",
    "#print(\"Reshaped Grid Value Function:\")\n",
    "#print(v.reshape((30,30)))\n",
    "#print(\"\")\n",
    "\n",
    "\n",
    "pol = np.reshape(np.argmax(policy, axis=1), (30,30))\n",
    "\n",
    "# pol[x][y] = Policy for the agent if agent is in field x and minataur is in field y.\n",
    "# Ex. Pol[:,13].reshape((5,6)) shows the agent behavior in each field if the minataurs was sitatued in field nr 13.\n",
    "pol[:,13].reshape((5,6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "# WORK IN PROGRESS\n",
    "\n",
    "def simulation(policy,T, start_states):\n",
    "    \n",
    "    \n",
    "    \"\"\"Simulating a policy in the Random Minotaur world. \n",
    "    Args : \n",
    "        policy - Policy in matrix form\n",
    "        T - maximum time allowed in the maze\n",
    "        start_states - A tuple (agent_state, minataur state) of the start state of the agent and the minotaur\n",
    "    \n",
    "    Returns : \n",
    "        A list of tupels [(agent_state, minataur state)] of the moves made by the agent and the minataur. \n",
    "        If_sucessful - Booelan if the agent sucessfully exit the maze before T\n",
    "    \"\"\"\n",
    "    t = 0\n",
    "    \n",
    "    is_sucessful = False\n",
    "    \n",
    "    current_agent_state = start_states[0]\n",
    "    current_minataur_state = start_states[1]\n",
    "    \n",
    "    while t <= T:\n",
    "        minataur_next_move = randint(0,3)\n",
    "        \n",
    "        # minataur_state = \n",
    "        \n",
    "        \n",
    "        agnet_next_move = policy[agent_state, minataur_state]\n",
    "        \n",
    "        #If the agent in the next move reaches the terminal state\n",
    "        if agent_next_move == 28:\n",
    "            is_sucessful = True\n",
    "        \n",
    "    \n",
    "    return is_sucessful\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
