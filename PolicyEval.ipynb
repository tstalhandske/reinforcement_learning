{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pprint\n",
    "import sys\n",
    "if \"../\" not in sys.path:\n",
    "  sys.path.append(\"../\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OBS CURRENTLY NOT IN USE\n",
    "\n",
    "# Minataur blocked paths\n",
    "\n",
    "Blocked_minataur = {\n",
    "    0: [],\n",
    "    1: [x for x in range(0,6)],\n",
    "    2: [5,11,17,23,29],\n",
    "    3: [x for x in range(24,30)],\n",
    "    4: [0,6,12,18,24]\n",
    "}\n",
    "\n",
    "Blocked_agent = {\n",
    "    0: [],\n",
    "    1: [16,17,25,26,27,28,0,1,2,3,4,5],\n",
    "    2: [1,7,13,9,15,27,5,11,17,23,29],\n",
    "    3: [10,11,19,20,21,22,24,25,26,27,28,29],\n",
    "    4: [2,8,14,10,16,28,0,6,12,18,24]\n",
    "}\n",
    "\n",
    "agent_moves = {}\n",
    "minataur_moves = {}\n",
    "\n",
    "grid = np.arange(30).reshape((5,6))\n",
    "it = np.nditer(grid, flags=['multi_index'])\n",
    "\n",
    "while not it.finished:\n",
    "    s = it.iterindex\n",
    "    x,y  = it.multi_index\n",
    "    \n",
    "    moves_agent = []\n",
    "    moves_minataur = []\n",
    "    for move in range(4):\n",
    "        \n",
    "        if not s in Blocked_agent[move]:\n",
    "            moves_agent.append(move)\n",
    "        \n",
    "        if not s in Blocked_minataur[move]:\n",
    "            moves_minataur.append(move)\n",
    "            \n",
    "    moves_agent.append(4)\n",
    "    \n",
    "    agent_moves[s] = moves_agent\n",
    "    minataur_moves[s] = moves_minataur\n",
    "    \n",
    "    it.iternext()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "#from gym.envs.toy_text import discrete\n",
    "\n",
    "STAY = 0\n",
    "UP = 1\n",
    "RIGHT = 2\n",
    "DOWN = 3\n",
    "LEFT = 4\n",
    "\n",
    "\n",
    "class GridworldEnv():\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    You are an agent on an MxN grid and your goal is to reach the terminal\n",
    "    state at the bottom right corner. There is also an minatours that is moving randomly\n",
    "    unifromaly, if he catches you, you will be eated. \n",
    "\n",
    "    You can take actions in each direction (UP=0, RIGHT=1, DOWN=2, LEFT=3, STAY=4).\n",
    "    Actions going off the edge leave you in your current state with a reward -1.\n",
    "    Actions going into a wall, leave you in your current state with a reward -1. \n",
    "    This make the agent learn not to bump into walls or edges. \n",
    "    \n",
    "    You receive a reward of -1 at each step until you reach a terminal state.\n",
    "    Your recive a reward of -10 if the minatours catches you\n",
    "    \n",
    "    There are 30*30 number of states, one state represent the tuple (agent_state, minataur_state) in the enviroment\n",
    "    the full set of states descirbe all possible combination of theese states. \n",
    "    \n",
    "    \n",
    "    \n",
    "    # LEFT TO FIX:\n",
    "        Alter the proababilities and next states when the minataur is at an edge. Now prob=0.25 for any of the directions.\n",
    "        Diffrent discount factors? \n",
    "            More important to avoid the minataurs then getting closer to the goal so discount factor < 1 is necessary.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    metadata = {'render.modes': ['human', 'ansi']}\n",
    "\n",
    "    def __init__(self, shape=[5,6]):\n",
    "\n",
    "        self.shape = shape\n",
    "    \n",
    "        # Number of states\n",
    "        self.nS = 900\n",
    "        #Number of actions\n",
    "        self.nA = 5\n",
    "        \n",
    "        #Shape of the maze\n",
    "        MAX_Y = shape[0] #5\n",
    "        MAX_X = shape[1] #6\n",
    "    \n",
    "        #Translation Matrix, used to find the state index\n",
    "        state_index = np.arange(self.nS).reshape((30,30))\n",
    "        \n",
    "        # Minataur blocked paths. Minataur can't go outside edges\n",
    "        Edge_up = [x for x in range(0,6)]\n",
    "        Edge_right = [5,11,17,23,29]\n",
    "        Edge_down = [x for x in range(24,30)]\n",
    "        Edge_left = [0,6,12,18,24]\n",
    "        \n",
    "\n",
    "        #Agents blocked paths. Agent can't go outside edges or into walls.\n",
    "        Blocked_agent_up = [16,17,25,26,27,28,0,1,2,3,4,5]\n",
    "        Blocked_agent_right = [1,7,13,9,15,27,5,11,17,23,29]\n",
    "        Blocked_agent_down = [10,11,19,20,21,22,24,25,26,27,28,29]\n",
    "        Blocked_agent_left = [2,8,14,10,16,28,0,6,12,18,24]\n",
    "        \n",
    "        #Init of transition matrix\n",
    "        P = {}\n",
    "\n",
    "        #Matrix (30*30) with all numbered possible states\n",
    "        grid = np.arange(self.nS).reshape((30,30))\n",
    "        #Iteration tool, instead of using two for loops\n",
    "        it = np.nditer(grid, flags=['multi_index'])\n",
    "\n",
    "        while not it.finished:\n",
    "\n",
    "            s = it.iterindex\n",
    "            # x= agent current state (0-29) , y = minataur current state  (0-29)\n",
    "            x,y  = it.multi_index\n",
    "\n",
    "            #Stopping condition, when have reached terminal state (either end of maze or eaten)\n",
    "            is_done = lambda x: x == 28\n",
    "            is_eaten = lambda x,y: x == y\n",
    "            #Negative reward for everytime step we are not in the terminal state\n",
    "            reward = 0.0 if is_done(x) else -1.0\n",
    "            #Negative reward if out state and the minataurs is the same(we have been eaten)\n",
    "            if is_eaten(x,y) == True:\n",
    "                reward = -1\n",
    "\n",
    "                        \n",
    "            P[s] = {a : [[]] for a in range(self.nA)}\n",
    "\n",
    "            # We're stuck in a terminal state if the agent is in the terminal node, no matter where\n",
    "            # the minataur is\n",
    "            if is_done(x) or is_eaten(x,y):\n",
    "                P[s][UP] = [([1.0], [s], [reward], True)]\n",
    "                P[s][RIGHT] = [([1.0], [s], [reward], True)]\n",
    "                P[s][DOWN] = [([1.0], [s], [reward], True)]\n",
    "                P[s][LEFT] = [([1.0], [s], [reward], True)]\n",
    "                P[s][STAY] = [([1.0], [s], [reward], True)]\n",
    "            # Not a terminal state\n",
    "            else:\n",
    "                # First determine states depending on out choice of action\n",
    "                #If we go up when we are at the top of maze, we end up the same pos.\n",
    "                #If we go up when we are at any position in Walls_Up, we end up in same pos\n",
    "                ns_up = x if x in Blocked_agent_up else x - MAX_X\n",
    "                ns_right = x if x in Blocked_agent_right else x + 1\n",
    "                ns_down = x if x in Blocked_agent_down else x + MAX_X\n",
    "                ns_left = x if x in Blocked_agent_left else x - 1\n",
    "\n",
    "                # Determine next state of the minataurs, 4 possibilies\n",
    "                ns_m_up = y if y in Edge_up else y - MAX_X\n",
    "                ns_m_right = y if y in Edge_right else y + 1\n",
    "                ns_m_down = y if y in Edge_down else y + MAX_X\n",
    "                ns_m_left = y if y in Edge_left else y -1\n",
    "\n",
    "                # After an action a, we can be in four possible states. A total of 16 \n",
    "            \n",
    "                agent_next_states = [x,ns_up, ns_right, ns_down, ns_left]\n",
    "                minataur_next_states = [y,ns_m_up, ns_m_right,ns_m_down, ns_m_left]\n",
    "     \n",
    "                action = 0\n",
    "                \n",
    "                # Buliding transition matrix P\n",
    "                for ns in agent_next_states:\n",
    "                    prob_list = []\n",
    "                    next_state_list = []\n",
    "                    reward_list = []\n",
    "\n",
    "                    for ns_m in minataur_next_states:\n",
    "                        reward = -1\n",
    "                        ns_index = state_index[ns][ns_m]\n",
    "                        next_state_list.append(ns_index)\n",
    "                        prob_list.append(0.25)\n",
    "                        \n",
    "                        # If the agent next state == minataurs next state -> NO GOOD! \n",
    "                        if ns == ns_m:\n",
    "                            reward=-50\n",
    "                        # If the agent next state == minataurs current state -> EVEN WORSE!\n",
    "                        if ns ==y:\n",
    "                            reward = -50\n",
    "\n",
    "                        reward_list.append(reward)\n",
    "                    \n",
    "                    \n",
    "                    P[s][action] = [(prob_list,next_state_list, reward_list, is_done(ns))] \n",
    "                    action += 1\n",
    "                    \n",
    "\n",
    "            it.iternext()\n",
    "\n",
    "        # Initial state distribution is uniform\n",
    "        isd = np.ones(self.nS) / self.nS\n",
    "\n",
    "        self.P = P\n",
    "\n",
    "        #super(GridworldEnv, self).__init__(nS, nA, P, isd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = pprint.PrettyPrinter(indent=2)\n",
    "env = GridworldEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, theta=0.0001, discount_factor=1):\n",
    "    \"\"\"\n",
    "    Value Iteration Algorithm.\n",
    "    \n",
    "    Args:\n",
    "        env:env.P represents the transition probabilities of the environment.\n",
    "            env.P[s][a] is a list of transition tuples (prob, next_state, reward, done).\n",
    "            env.nS is a number of states in the environment. \n",
    "            env.nA is a number of actions in the environment.\n",
    "        theta: We stop evaluation once our value function change is less than theta for all states.\n",
    "        discount_factor: Gamma discount factor.\n",
    "        \n",
    "    Returns:\n",
    "        A tuple (policy, V) of the optimal policy and the optimal value function.\n",
    "    \"\"\"\n",
    "    \n",
    "    def one_step_lookahead(state, V):\n",
    "        \"\"\"\n",
    "        Helper function to calculate the value for all action in a given state.\n",
    "        \n",
    "        Args:\n",
    "            state: The state to consider (int)\n",
    "            V: The value to use as an estimator, Vector of length env.nS\n",
    "        \n",
    "        Returns:\n",
    "            A vector of length env.nA containing the expected value of each action.\n",
    "        \"\"\"\n",
    "        A = np.zeros(env.nA)\n",
    "        for a in range(env.nA):\n",
    "            for prob, next_state, reward, done in env.P[state][a]:\n",
    "                #Prob a vector for the probabilities of the nex_state\n",
    "                #Prob a vector for the possible next states\n",
    "                # Rewards in thoose possible next states\n",
    "               \n",
    "                for i in range(len(prob)):\n",
    "                    A[a] += prob[i] * (reward[i] + discount_factor * V[next_state[i]])\n",
    "        \n",
    "               \n",
    "        return A\n",
    "    \n",
    "    V = np.zeros(env.nS)\n",
    "    maxIter = 50\n",
    "    i = 0\n",
    "    while True:\n",
    "        # Stopping condition\n",
    "        delta = 0\n",
    "        # Update each state...\n",
    "        for s in range(env.nS):\n",
    "            \n",
    "            # Do a one-step lookahead to find the best action\n",
    "            A = one_step_lookahead(s, V)\n",
    "            best_action_value = np.max(A)\n",
    "            # Calculate delta across all states seen so far\n",
    "            delta = max(delta, np.abs(best_action_value - V[s]))\n",
    "            # Update the value function. Ref: Sutton book eq. 4.10. \n",
    "            V[s] = best_action_value        \n",
    "        # Check if we can stop \n",
    "        \n",
    "        i +=1\n",
    "        print(\"Delta-Value \", delta)\n",
    "        if delta < theta:\n",
    "            break\n",
    "        if i >=maxIter:\n",
    "            break\n",
    "        \n",
    "    \n",
    "    # Create a deterministic policy using the optimal value function\n",
    "    policy = np.zeros([env.nS, env.nA])\n",
    "    for s in range(env.nS):\n",
    "        # One step lookahead to find the best action for this state\n",
    "        A = one_step_lookahead(s, V)\n",
    "        best_action = np.argmax(A)\n",
    "        # Always take the best action\n",
    "        policy[s, best_action] = 1.0\n",
    "    \n",
    "    return policy, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delta-Value  14.65081787109375\n",
      "Delta-Value  10.793346405029297\n",
      "Delta-Value  9.814752086997032\n",
      "Delta-Value  10.324775271117687\n",
      "Delta-Value  13.784000312157048\n",
      "Delta-Value  19.297146580315086\n",
      "Delta-Value  24.32536731507615\n",
      "Delta-Value  24.038909629176615\n",
      "Delta-Value  29.654502417550873\n",
      "Delta-Value  28.6487036303767\n",
      "Delta-Value  28.92501404083208\n",
      "Delta-Value  27.11233549083431\n",
      "Delta-Value  27.95236099669279\n",
      "Delta-Value  29.5798285635679\n",
      "Delta-Value  25.95059888279502\n",
      "Delta-Value  24.168906950447195\n",
      "Delta-Value  25.677222356079938\n",
      "Delta-Value  22.53593653270218\n",
      "Delta-Value  11.481615183760198\n",
      "Delta-Value  7.080640778164195\n",
      "Delta-Value  5.290019529269159\n",
      "Delta-Value  4.380557421742822\n",
      "Delta-Value  4.126440118038545\n",
      "Delta-Value  4.088912483172749\n",
      "Delta-Value  4.069679851010619\n",
      "Delta-Value  4.059827772028143\n",
      "Delta-Value  4.054783484535847\n",
      "Delta-Value  4.052202117991612\n",
      "Delta-Value  4.050881796770113\n",
      "Delta-Value  4.050206810069085\n",
      "Delta-Value  4.049861901771067\n",
      "Delta-Value  4.049685739641291\n",
      "Delta-Value  4.049595804450632\n",
      "Delta-Value  4.049549909756934\n",
      "Delta-Value  4.00987960747176\n",
      "Delta-Value  3.7879780439427577\n",
      "Delta-Value  3.7664576964985486\n",
      "Delta-Value  3.7605176111086394\n",
      "Delta-Value  3.7588865344737314\n",
      "Delta-Value  3.7584407233139814\n",
      "Delta-Value  3.758319370846152\n",
      "Delta-Value  3.758286456949179\n",
      "Delta-Value  3.758277557531585\n",
      "Delta-Value  3.7582751573000337\n",
      "Delta-Value  3.7582745110419182\n",
      "Delta-Value  3.758274337115381\n",
      "Delta-Value  3.7582742902286554\n",
      "Delta-Value  3.7582742775215934\n",
      "Delta-Value  3.758274274037092\n",
      "Delta-Value  3.758274273059442\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[3, 3, 2, 3, 4, 4],\n",
       "       [3, 4, 2, 3, 1, 1],\n",
       "       [3, 0, 2, 3, 2, 3],\n",
       "       [2, 2, 2, 2, 2, 3],\n",
       "       [1, 4, 4, 4, 0, 4]], dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "policy, v = value_iteration(env)\n",
    "\n",
    "#print(\"Policy Probability Distribution:\")\n",
    "#print(policy)\n",
    "#print(\"\")\n",
    "\n",
    "#print(\"Reshaped Grid Policy (0=up, 1=right, 2=down, 3=left):\")\n",
    "#print(np.reshape(np.argmax(policy, axis=1), (30,30)))\n",
    "#print(\"\")\n",
    "\n",
    "#print(\"Value Function:\")\n",
    "#print(v)\n",
    "#print(\"\")\n",
    "\n",
    "#print(\"Reshaped Grid Value Function:\")\n",
    "#print(v.reshape((30,30)))\n",
    "#print(\"\")\n",
    "\n",
    "\n",
    "pol = np.reshape(np.argmax(policy, axis=1), (30,30))\n",
    "\n",
    "# pol[x][y] = Policy for the agent if agent is in field x and minataur is in field y.\n",
    "# Ex. Pol[:,13].reshape((5,6)) shows the agent behavior in each field if the minataurs was sitatued in field nr 13.\n",
    "pol[:,13].reshape((5,6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_state(current_state, action):\n",
    "\n",
    "    #Shape of the maze\n",
    "    MAX_Y = 5\n",
    "    MAX_X = 6\n",
    "    \n",
    " \n",
    "    if action == STAY:\n",
    "        next_state = current_state\n",
    "    \n",
    "    if action == UP:\n",
    "        next_state = current_state - MAX_X\n",
    "        \n",
    "    if action == RIGHT:\n",
    "        next_state = current_state + 1\n",
    "    \n",
    "    if action == DOWN:\n",
    "        next_state = current_state + MAX_X\n",
    "        \n",
    "    if action == LEFT:\n",
    "        next_state = current_state - 1\n",
    "    \n",
    "    return next_state\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "# WORK IN PROGRESS\n",
    "\n",
    "def simulation(policy,T, start_states, allowed_stay):\n",
    "    \n",
    "    \n",
    "    \"\"\"Simulating a policy in the Random Minotaur world. \n",
    "    Args : \n",
    "        policy - Policy in matrix form\n",
    "        T - maximum time allowed in the maze\n",
    "        start_states - A tuple (agent_state, minataur state) of the start state of the agent and the minotaur\n",
    "        allowed stay - Booelan, if the minataur is allwed to stand still or no\n",
    "        \n",
    "    Returns : \n",
    "        A list of tupels [(agent_state, minataur state)] of the moves made by the agent and the minataur. \n",
    "        If_sucessful - Booelan if the agent sucessfully exit the maze before T\n",
    "    \"\"\"\n",
    "    t = 0\n",
    "    \n",
    "    is_sucessful = False\n",
    "    \n",
    "    \n",
    "    Blocked_minataur = {\n",
    "    0: [], # You can always stay\n",
    "    1: [x for x in range(0,6)],\n",
    "    2: [5,11,17,23,29],\n",
    "    3: [x for x in range(24,30)],\n",
    "    4: [0,6,12,18,24]\n",
    "}\n",
    "\n",
    "    current_agent_state = start_states[0]\n",
    "    current_minataur_state = start_states[1]\n",
    "    \n",
    "    states = []\n",
    "    \n",
    "    while t <= T:\n",
    "        \n",
    "        minataur_next_move = randint(0,4) if allowed_stay else randint(1,4)\n",
    "        while current_minataur_state in Blocked_minataur[minataur_next_move]:\n",
    "            minataur_next_move = randint(0,4) if allowed_stay else randint(1,4)\n",
    "        \n",
    "        # Get the best move for the agent\n",
    "        agent_next_move = policy[current_agent_state, current_minataur_state]\n",
    "        \n",
    "        # Get the next states\n",
    "        minataur_next_state = get_next_state(current_minataur_state, minataur_next_move)\n",
    "        agent_next_state = get_next_state(current_agent_state, agent_next_move)\n",
    "       \n",
    "        current_agent_state = agent_next_state\n",
    "        current_minataur_state = minataur_next_state\n",
    "        \n",
    "        #If the agent in the next move reaches the terminal state\n",
    "        if current_agent_state == 28:\n",
    "            is_sucessful = True\n",
    "    \n",
    "        \n",
    "        states.append((agent_next_state, minataur_next_state))\n",
    "        \n",
    "        t += 1\n",
    "        \n",
    "        \n",
    "    return (states, is_sucessful)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 0.469\n",
      "10 0.704\n",
      "11 0.846\n",
      "12 0.906\n",
      "13 0.958\n",
      "14 0.98\n",
      "15 0.986\n",
      "16 0.993\n",
      "17 0.996\n",
      "18 0.996\n",
      "19 0.997\n",
      "20 1.0\n",
      "21 1.0\n",
      "22 1.0\n",
      "23 0.999\n",
      "24 0.999\n",
      "25 1.0\n",
      "26 1.0\n",
      "27 1.0\n",
      "28 1.0\n",
      "29 1.0\n",
      "30 1.0\n",
      "31 1.0\n",
      "32 1.0\n",
      "33 1.0\n",
      "34 1.0\n",
      "35 1.0\n",
      "36 1.0\n",
      "37 1.0\n",
      "38 1.0\n",
      "39 1.0\n",
      "40 1.0\n",
      "41 1.0\n",
      "42 1.0\n",
      "43 1.0\n",
      "44 1.0\n",
      "45 1.0\n",
      "46 1.0\n",
      "47 1.0\n",
      "48 1.0\n",
      "49 1.0\n"
     ]
    }
   ],
   "source": [
    "T = 15\n",
    "start_agent = 0\n",
    "start_minataur = 28\n",
    "\n",
    "counts = []\n",
    "\n",
    "for T in range(9,50):\n",
    "    count = 0\n",
    "    for i in range(1000):\n",
    "        \n",
    "        states, is_sucessful = simulation(pol, T, [start_agent, start_minataur], True)\n",
    "        \n",
    "        if is_sucessful:\n",
    "            count += 1\n",
    "    counts.append(count/1000)\n",
    "    print(T, count/1000)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.469,\n",
       " 0.704,\n",
       " 0.846,\n",
       " 0.906,\n",
       " 0.958,\n",
       " 0.98,\n",
       " 0.986,\n",
       " 0.993,\n",
       " 0.996,\n",
       " 0.996,\n",
       " 0.997,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.999,\n",
       " 0.999,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Probability of exiting the maze as a function of T')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "times = [T for T in range(9,50)]\n",
    "\n",
    "# Plot the probability of exiting the maze as a function of the allowed time in the maze\n",
    "plt.plot(times, counts)\n",
    "plt.title(\"Probability of exiting the maze as a function of T\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Probability of exiting the maze as a function of T')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Doing a bit more prettier curve fit\n",
    "\n",
    "z = np.polyfit(times,counts,10)\n",
    "f = np.poly1d(z)\n",
    "\n",
    "times_new = np.linspace(times[0], times[-1], 1000)\n",
    "counts_new = f(times_new)\n",
    "\n",
    "plt.plot(times, counts,'o', times_new, counts_new)\n",
    "plt.title(\"Probability of exiting the maze as a function of T\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of sucess  0.742\n"
     ]
    }
   ],
   "source": [
    "# Now assuming life is geometrically distributed with mean 30\n",
    "# The diffrence now is that we wish to derive a policy minimising the expected\n",
    "#time in the maze. \n",
    "#\n",
    "\n",
    "start_agent = 0\n",
    "start_minataur = 28\n",
    "\n",
    "counts = []\n",
    "count = 0\n",
    "for t in range(1000):\n",
    "    T = np.random.geometric(1/30)\n",
    "\n",
    "    states, is_sucessful = simulation(pol, T, [start_agent, start_minataur], True)\n",
    "\n",
    "    if is_sucessful:\n",
    "        count += 1            \n",
    "    \n",
    "\n",
    "print(\"Probability of sucess \", count/1000)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "\n",
    "def create_grid(event=None):\n",
    "    w = c.winfo_width() # Get current width of canvas\n",
    "    h = c.winfo_height() # Get current height of canvas\n",
    "    c.delete('grid_line') # Will only remove the grid_line\n",
    "\n",
    "    # Creates all vertical lines at intevals of 100\n",
    "    for i in range(0, w, 100):\n",
    "        \n",
    "        if i == 200:\n",
    "            c.create_line([(i, 0), (i, 3*h/5)], tag='grid_line', width=15)\n",
    "            c.create_line([(i, 3*h/5), (i, h)], tag='grid_line')\n",
    "        elif i == 400:\n",
    "            c.create_line([(i, 0), (i, h/5)], tag='grid_line')\n",
    "            c.create_line([(i, h/5), (i, 3*h/5)], tag='grid_line', width=15)\n",
    "            c.create_line([(i, 3*h/5), (i, 4*h/5)], tag='grid_line')\n",
    "            \n",
    "            c.create_line([(i, 4*h/5), (i, h)], tag='grid_line', width = 15)\n",
    "           \n",
    "        else:\n",
    "            c.create_line([(i, 0), (i, h)], tag='grid_line')\n",
    "            \n",
    "\n",
    "    # Creates all horizontal lines at intevals of 100\n",
    "    for i in range(0, h, 100):\n",
    "        \n",
    "        if i == 400:\n",
    "            c.create_line([(0, i), (w, i)], tag='grid_line')\n",
    "            c.create_line([(w/6, i), (5*w/6, i)], tag='grid_line', width=15)\n",
    "            \n",
    "        elif i ==200:\n",
    "            c.create_line([(0, i), (w, i)], tag='grid_line')\n",
    "            c.create_line([(4*w/6, i), (6*w/6, i)], tag='grid_line', width=15)\n",
    "            \n",
    "        else: \n",
    "            c.create_line([(0, i), (w, i)], tag='grid_line')\n",
    "    \n",
    "    \n",
    "                \n",
    "\n",
    "root = tk.Tk()\n",
    "\n",
    "c = tk.Canvas(root, height=500, width=600, bg='white')\n",
    "c.pack(fill=tk.BOTH, expand=True)\n",
    "\n",
    "c.bind('<Configure>', create_grid)\n",
    "\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
