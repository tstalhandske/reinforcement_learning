{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pprint\n",
    "import sys\n",
    "if \"../\" not in sys.path:\n",
    "  sys.path.append(\"../\") \n",
    "#from gridworld import GridworldEnv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minataur blocked paths\n",
    "\n",
    "Blocked_minataur = {\n",
    "    0: [x for x in range(0,6)],\n",
    "    1: [5,11,17,23,29],\n",
    "    2: [x for x in range(24,30)],\n",
    "    3: [0,6,12,18,24]\n",
    "}\n",
    "\n",
    "Blocked_agent = {\n",
    "    0: [16,17,25,26,27,28,0,1,2,3,4,5],\n",
    "    1: [1,7,13,9,15,27,5,11,17,23,29],\n",
    "    2: [10,11,19,20,21,22,24,25,26,27,28,29],\n",
    "    3: [2,8,14,10,16,28,0,6,12,18,24]\n",
    "}\n",
    "\n",
    "agent_moves = {}\n",
    "minataur_moves = {}\n",
    "\n",
    "grid = np.arange(30).reshape((5,6))\n",
    "it = np.nditer(grid, flags=['multi_index'])\n",
    "\n",
    "while not it.finished:\n",
    "    s = it.iterindex\n",
    "    x,y  = it.multi_index\n",
    "    \n",
    "    moves_agent = []\n",
    "    moves_minataur = []\n",
    "    for move in range(4):\n",
    "        \n",
    "        if not s in Blocked_agent[move]:\n",
    "            moves_agent.append(move)\n",
    "        \n",
    "        if not s in Blocked_minataur[move]:\n",
    "            moves_minataur.append(move)\n",
    "            \n",
    "    moves_agent.append(4)\n",
    "    \n",
    "    agent_moves[s] = moves_agent\n",
    "    minataur_moves[s] = moves_minataur\n",
    "    \n",
    "    it.iternext()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "from gym.envs.toy_text import discrete\n",
    "\n",
    "STAY = 0\n",
    "UP = 1\n",
    "RIGHT = 2\n",
    "DOWN = 3\n",
    "LEFT = 4\n",
    "\n",
    "\n",
    "class GridworldEnv(discrete.DiscreteEnv):\n",
    "    \"\"\"\n",
    "    You are an agent on an MxN grid and your goal is to reach the terminal\n",
    "    state at the bottom right corner. There is also an minatours that is moving randomly\n",
    "    unifromaly, if he catches you, you will be eated. \n",
    "\n",
    "    You can take actions in each direction (UP=0, RIGHT=1, DOWN=2, LEFT=3, STAY=4).\n",
    "    Actions going off the edge leave you in your current state.\n",
    "    Actions going into a wall, leave you in your current state. \n",
    "    You receive a reward of -1 at each step until you reach a terminal state.\n",
    "    Your recive a reward of -100 if the minatours catches you\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    metadata = {'render.modes': ['human', 'ansi']}\n",
    "\n",
    "    def __init__(self, shape=[5,6]):\n",
    "\n",
    "        self.shape = shape\n",
    "\n",
    "        nS = 900\n",
    "        nA = 5\n",
    "\n",
    "        MAX_Y = shape[0]\n",
    "        MAX_X = shape[1]\n",
    "    \n",
    "        #Translation Matrix\n",
    "        state_index = np.arange(nS).reshape((30,30))\n",
    "        \n",
    "        # Minataur blocked paths\n",
    "        Edge_up = [x for x in range(0,6)]\n",
    "        Edge_right = [5,11,17,23,29]\n",
    "        Edge_down = [x for x in range(24,30)]\n",
    "        Edge_left = [0,6,12,18,24]\n",
    "        \n",
    "\n",
    "        #Agents blocked paths\n",
    "        Blocked_agent_up = [16,17,25,26,27,28,0,1,2,3,4,5]\n",
    "        Blocked_agent_right = [1,7,13,9,15,27,5,11,17,23,29]\n",
    "        Blocked_agent_down = [10,11,19,20,21,22,24,25,26,27,28,29]\n",
    "        Blocked_agent_left = [2,8,14,10,16,28,0,6,12,18,24]\n",
    "        \n",
    "        P = {}\n",
    "\n",
    "        #Matrix (30*30) with all numbered possible states\n",
    "        grid = np.arange(nS).reshape((30,30))\n",
    "        #Iteration tool, instead of using two for loops\n",
    "        it = np.nditer(grid, flags=['multi_index'])\n",
    "\n",
    "        while not it.finished:\n",
    "\n",
    "            s = it.iterindex\n",
    "          \n",
    "            x,y  = it.multi_index\n",
    "\n",
    "            #Stopping condition, when have reached terminal state (either end of maze or eaten)\n",
    "            is_done = lambda x: x == 28\n",
    "            is_eaten = lambda x,y: x == y\n",
    "            #Negative reward for everytime step we are not in the terminal state\n",
    "            reward = 0.0 if is_done(x) else -1.0\n",
    "            #Negative reward if out state and the minataurs is the same(we have been eaten)\n",
    "            if is_eaten(x,y) == True:\n",
    "                reward = -1\n",
    "\n",
    "            #print(\"Index of state:\", index)\n",
    "            \n",
    "                        \n",
    "            P[s] = {a : [[]] for a in range(nA)}\n",
    "\n",
    "            # We're stuck in a terminal state if the agent is in the terminal node, no matter where\n",
    "            # the minataur is\n",
    "            if is_done(x) or is_eaten(x,y):\n",
    "                P[s][UP] = [([1.0], [s], [reward], True)]\n",
    "                P[s][RIGHT] = [([1.0], [s], [reward], True)]\n",
    "                P[s][DOWN] = [([1.0], [s], [reward], True)]\n",
    "                P[s][LEFT] = [([1.0], [s], [reward], True)]\n",
    "                P[s][STAY] = [([1.0], [s], [reward], True)]\n",
    "            # Not a terminal state\n",
    "            else:\n",
    "                # First determine states depending on out choice of action\n",
    "                #If we go up when we are at the top of maze, we end up the same pos.\n",
    "                #If we go up when we are at any position in Walls_Up, we end up in same pos\n",
    "                ns_up = x if x in Blocked_agent_up else x - MAX_X\n",
    "                ns_right = x if x in Blocked_agent_right else x + 1\n",
    "                ns_down = x if x in Blocked_agent_down else x + MAX_X\n",
    "                ns_left = x if x in Blocked_agent_left else x - 1\n",
    "\n",
    "                # Determine next state of the minataurs, 4 possibilies\n",
    "                ns_m_up = y if y in Edge_up else y - MAX_X\n",
    "                ns_m_right = y if y in Edge_right else y + 1\n",
    "                ns_m_down = y if y in Edge_down else y + MAX_X\n",
    "                ns_m_left = y if y in Edge_left else y -1\n",
    "\n",
    "                # After an action a, we can be in four possible states. A total of 16 \n",
    "                \n",
    "                list1 = [x,ns_up, ns_right, ns_down, ns_left]\n",
    "                list2 = [y,ns_m_up, ns_m_right,ns_m_down, ns_m_left]\n",
    "     \n",
    "                action = 0\n",
    "        \n",
    "                for ns in list1:\n",
    "                    prob_list = []\n",
    "                    next_state_list = []\n",
    "                    reward_list = []\n",
    "\n",
    "                    for ns_m in list2:\n",
    "                        reward = -1\n",
    "                        ns_index = state_index[ns][ns_m]\n",
    "                        next_state_list.append(ns_index)\n",
    "                        prob_list.append(0.25)\n",
    "                        \n",
    "                        if ns == ns_m:\n",
    "                            reward=-50\n",
    "                        if ns ==y:\n",
    "                            reward = -50\n",
    "                        \n",
    "                        #if action == 4:\n",
    "                        #    reward = -0.9\n",
    "\n",
    "                        reward_list.append(reward)\n",
    "                    \n",
    "                    \n",
    "                    P[s][action] = [(prob_list,next_state_list, reward_list, is_done(ns))] \n",
    "                    action += 1\n",
    "                    \n",
    "\n",
    "            it.iternext()\n",
    "\n",
    "        # Initial state distribution is uniform\n",
    "        isd = np.ones(nS) / nS\n",
    "\n",
    "        # We expose the model of the environment for educational purposes\n",
    "        # This should not be used in any model-free learning algorithm\n",
    "        self.P = P\n",
    "\n",
    "        super(GridworldEnv, self).__init__(nS, nA, P, isd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = pprint.PrettyPrinter(indent=2)\n",
    "env = GridworldEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.299199999999999\n",
      "7.717475840000002\n",
      "4.911613618176002\n",
      "3.8355623727104025\n",
      "3.3701874608668927\n",
      "2.942129985484179\n",
      "2.745033525083741\n",
      "2.63674204302173\n",
      "2.5071180084190274\n",
      "2.3419993491561435\n",
      "2.003428861003542\n",
      "1.7251384061422215\n",
      "1.4966763996774972\n",
      "1.290543651172399\n",
      "1.1070173746931893\n",
      "0.9382924071209331\n",
      "0.7925864375071257\n",
      "0.6679748990804697\n",
      "0.5620065013847366\n",
      "0.47222752196004336\n",
      "0.3963671980727881\n",
      "0.3323969050446749\n",
      "0.2785386391014981\n",
      "0.23325226159279566\n",
      "0.1952141397933076\n",
      "0.16329305063859323\n",
      "0.13652619441325697\n",
      "0.11409669069735173\n",
      "0.09531316114840394\n",
      "0.07959159130469828\n",
      "0.0664394425849153\n",
      "0.05544187213526186\n",
      "0.046249866116276905\n",
      "0.03857007502061549\n",
      "0.03215614214250451\n",
      "0.02680132915808997\n",
      "0.022332260469035248\n",
      "0.01860362731937215\n",
      "0.015493711914409403\n",
      "0.012900609877462443\n",
      "0.010739045909524236\n",
      "0.008937692293969235\n",
      "0.0074369129159777\n",
      "0.006186866831939142\n",
      "0.005145915269380907\n",
      "0.004279284416512041\n",
      "0.003557943631015803\n",
      "0.0029576649114062548\n",
      "0.002458234769939338\n",
      "0.002042794148032101\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[3, 0, 2, 3, 4, 4],\n",
       "       [0, 4, 2, 3, 1, 1],\n",
       "       [3, 0, 2, 3, 2, 3],\n",
       "       [0, 2, 2, 2, 2, 3],\n",
       "       [1, 4, 4, 4, 0, 4]], dtype=int64)"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "policy, v = value_iteration(env)\n",
    "\n",
    "#print(\"Policy Probability Distribution:\")\n",
    "#print(policy)\n",
    "#print(\"\")\n",
    "\n",
    "#print(\"Reshaped Grid Policy (0=up, 1=right, 2=down, 3=left):\")\n",
    "#print(np.reshape(np.argmax(policy, axis=1), (30,30)))\n",
    "#print(\"\")\n",
    "\n",
    "#print(\"Value Function:\")\n",
    "#print(v)\n",
    "#print(\"\")\n",
    "\n",
    "#print(\"Reshaped Grid Value Function:\")\n",
    "#print(v.reshape((30,30)))\n",
    "#print(\"\")\n",
    "\n",
    "\n",
    "pol = np.reshape(np.argmax(policy, axis=1), (30,30))\n",
    "pol[:,13].reshape((5,6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = np.arange(900).reshape((30,30))\n",
    "x = np.argwhere(grid == state)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "193"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#Translation Matrix\n",
    "state_index = np.arange(900).reshape((30,30))\n",
    "state_index[6][13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [([0.25, 0.25, 0.25, 0.25, 0.25],\n",
       "   [7, 14, 19, 12, 13],\n",
       "   [-1, -1, -1, -1, -1],\n",
       "   False)],\n",
       " 1: [([0.25, 0.25, 0.25, 0.25, 0.25],\n",
       "   [217, 224, 229, 222, 223],\n",
       "   [-50, -1, -1, -1, -1],\n",
       "   False)],\n",
       " 2: [([0.25, 0.25, 0.25, 0.25, 0.25],\n",
       "   [367, 374, 379, 372, 373],\n",
       "   [-1, -1, -1, -50, -1],\n",
       "   False)],\n",
       " 3: [([0.25, 0.25, 0.25, 0.25, 0.25],\n",
       "   [187, 194, 199, 192, 193],\n",
       "   [-1, -1, -1, -1, -1],\n",
       "   False)],\n",
       " 4: [([0.25, 0.25, 0.25, 0.25, 0.25],\n",
       "   [187, 194, 199, 192, 193],\n",
       "   [-1, -1, -1, -1, -1],\n",
       "   False)]}"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.P[193]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob, next_state, reward, done = env.P[840][4][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, theta=0.0001, discount_factor=0.8):\n",
    "    \"\"\"\n",
    "    Value Iteration Algorithm.\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI env. env.P represents the transition probabilities of the environment.\n",
    "            env.P[s][a] is a list of transition tuples (prob, next_state, reward, done).\n",
    "            env.nS is a number of states in the environment. \n",
    "            env.nA is a number of actions in the environment.\n",
    "        theta: We stop evaluation once our value function change is less than theta for all states.\n",
    "        discount_factor: Gamma discount factor.\n",
    "        \n",
    "    Returns:\n",
    "        A tuple (policy, V) of the optimal policy and the optimal value function.\n",
    "    \"\"\"\n",
    "    \n",
    "    def one_step_lookahead(state, V):\n",
    "        \"\"\"\n",
    "        Helper function to calculate the value for all action in a given state.\n",
    "        \n",
    "        Args:\n",
    "            state: The state to consider (int)\n",
    "            V: The value to use as an estimator, Vector of length env.nS\n",
    "        \n",
    "        Returns:\n",
    "            A vector of length env.nA containing the expected value of each action.\n",
    "        \"\"\"\n",
    "        A = np.zeros(env.nA)\n",
    "        for a in range(env.nA):\n",
    "            for prob, next_state, reward, done in env.P[state][a]:\n",
    "                #Prob a vector for the probabilities of the nex_state\n",
    "                #Prob a vector for the possible next states\n",
    "                # Rewards in thoose possible next states\n",
    "               \n",
    "                for i in range(len(prob)):\n",
    "                    A[a] += prob[i] * (reward[i] + discount_factor * V[next_state[i]])\n",
    "        \n",
    "               \n",
    "        return A\n",
    "    \n",
    "    V = np.zeros(env.nS)\n",
    "    maxIter = 50\n",
    "    i = 0\n",
    "    while True:\n",
    "        # Stopping condition\n",
    "        delta = 0\n",
    "        # Update each state...\n",
    "        for s in range(env.nS):\n",
    "            \n",
    "            # Do a one-step lookahead to find the best action\n",
    "            A = one_step_lookahead(s, V)\n",
    "            best_action_value = np.max(A)\n",
    "            # Calculate delta across all states seen so far\n",
    "            delta = max(delta, np.abs(best_action_value - V[s]))\n",
    "            # Update the value function. Ref: Sutton book eq. 4.10. \n",
    "            V[s] = best_action_value        \n",
    "        # Check if we can stop \n",
    "        \n",
    "        i +=1\n",
    "        print(delta)\n",
    "        if delta < theta:\n",
    "            break\n",
    "        if i >=maxIter:\n",
    "            break\n",
    "        \n",
    "    \n",
    "    # Create a deterministic policy using the optimal value function\n",
    "    policy = np.zeros([env.nS, env.nA])\n",
    "    for s in range(env.nS):\n",
    "        # One step lookahead to find the best action for this state\n",
    "        A = one_step_lookahead(s, V)\n",
    "        best_action = np.argmax(A)\n",
    "        # Always take the best action\n",
    "        policy[s, best_action] = 1.0\n",
    "    \n",
    "    return policy, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-249-582641cb258a>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-249-582641cb258a>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    form random import randint\u001b[0m\n\u001b[1;37m              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "\n",
    "# WORK IN PROGRESS\n",
    "\n",
    "def simulation(policy,T, start_states):\n",
    "    \n",
    "    \n",
    "    \"\"\"Simulating a policy in the Random Minotaur world. \n",
    "    Args : \n",
    "        policy - Policy in matrix form\n",
    "        T - maximum time allowed in the maze\n",
    "        start_states - A tuple (agent_state, minataur state) of the start state of the agent and the minotaur\n",
    "    \n",
    "    Returns : \n",
    "        A list of tupels [(agent_state, minataur state)] of the moves made by the agent and the minataur. \n",
    "        If_sucessful - Booelan if the agent sucessfully exit the maze before T\n",
    "    \"\"\"\n",
    "    t = 0\n",
    "    \n",
    "    is_sucessful = False\n",
    "    \n",
    "    current_agent_state = start_states[0]\n",
    "    current_minataur_state = start_states[1]\n",
    "    \n",
    "    while t <= T:\n",
    "        minataur_next_move = randint(0,3)\n",
    "        \n",
    "        # minataur_state = \n",
    "        \n",
    "        \n",
    "        agnet_next_move = policy[agent_state, minataur_state]\n",
    "        \n",
    "        #If the agent in the next move reaches the terminal state\n",
    "        if agent_next_move == 28:\n",
    "            is_sucessful = True\n",
    "        \n",
    "    \n",
    "    return is_sucessful\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
