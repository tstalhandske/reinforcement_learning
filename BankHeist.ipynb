{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "STAY = 0\n",
    "UP = 1\n",
    "RIGHT = 2\n",
    "DOWN = 3\n",
    "LEFT = 4\n",
    "\n",
    "ACTIONS = [STAY, UP, RIGHT, DOWN, LEFT]\n",
    "STR_ACTIONS = ['STAY', 'UP', 'RIGHT', 'DOWN', 'LEFT']\n",
    "\n",
    "Edge_up = [x for x in range(0,6)]\n",
    "Edge_right = [5,11,17]\n",
    "Edge_down = [x for x in range(12,18)]\n",
    "Edge_left = [0,6,12]\n",
    "\n",
    "Banks = [0,5,12,17]\n",
    "Police_station = [8]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BankEnv():\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.shape = [3,6]\n",
    "        self.nS = 18*18\n",
    "        self.nA = 5\n",
    "        self.P = {}\n",
    "        \n",
    "        self.init_agent_state = Banks[0]\n",
    "        self.init_police_state = Police_station[0]\n",
    "        \n",
    "        self.state_index = np.arange(self.nS).reshape((18,18))\n",
    "        \n",
    "        self.init_state = self.state_index[self.init_agent_state][self.init_police_state]\n",
    "        \n",
    "        self.MAX_Y = self.shape[0]\n",
    "        self.MAX_X = self.shape[1]\n",
    "    \n",
    "    def get_column_line(self,x):    \n",
    "        \n",
    "        index = np.arange(18).reshape(self.shape)\n",
    "        coordinates = np.argwhere(index == x )\n",
    "        line = coordinates[0][0]\n",
    "        col = coordinates[0][1]\n",
    "        \n",
    "        return (line, col)\n",
    "    \n",
    "    def get_police_next_moves(self,agent_state, police_state):\n",
    "        \n",
    "        line_agent, col_agent = self.get_column_line(agent_state)\n",
    "        line_police, col_police = self.get_column_line(police_state)\n",
    "        \n",
    "        if line_agent == line_police:\n",
    "            \n",
    "            if agent_state > police_state:\n",
    "                # UP, DOWN, RIGHT\n",
    "                Moves = [UP, DOWN, RIGHT]\n",
    "            else:\n",
    "                # UP, DOWN, LEFT\n",
    "                Moves = [UP, DOWN, LEFT]\n",
    "                \n",
    "        \n",
    "        elif col_agent == col_police:\n",
    "            \n",
    "            if agent_state < police_state:   \n",
    "                Moves = [UP, RIGHT, LEFT]\n",
    "            else:\n",
    "                Moves = [DOWN, RIGHT, LEFT]\n",
    "                \n",
    "        else:\n",
    "            # If agent above and right\n",
    "            if agent_state < police_state and col_agent > col_police:\n",
    "                Moves = [UP, RIGHT]\n",
    "                \n",
    "            # If agent above and left\n",
    "            elif agent_state < police_state and col_agent < col_police:\n",
    "                Moves = [UP, LEFT]\n",
    "            \n",
    "            # If agent down and right\n",
    "            elif agent_state > police_state and col_agent > col_police:\n",
    "                Moves = [DOWN, RIGHT]\n",
    "            \n",
    "            # If agent down and left\n",
    "            elif agent_state > police_state and col_agent < col_police:\n",
    "                Moves = [DOWN, LEFT]\n",
    "                \n",
    "        return Moves\n",
    "    \n",
    "    def get_next_states(self,state, actions):\n",
    "        \n",
    "        next_states = {}\n",
    "\n",
    "        for action in actions:\n",
    "        \n",
    "            if action == UP and state not in Edge_up:\n",
    "                next_states[UP] = state - self.MAX_X\n",
    "\n",
    "            if action == RIGHT and state not in Edge_right:\n",
    "                next_states[RIGHT] = state + 1\n",
    "\n",
    "            if action == DOWN and state not in Edge_down:\n",
    "                next_states[DOWN] = state + self.MAX_X\n",
    "\n",
    "            if action == LEFT and state not in Edge_left:\n",
    "                next_states[LEFT] = state - 1\n",
    "                \n",
    "            if action == STAY:\n",
    "                next_states[STAY] = state\n",
    "            \n",
    "        \n",
    "        return next_states\n",
    "    \n",
    "    def create_transition_matrix(self):\n",
    "        \n",
    "        on_bank = lambda x: x in Banks\n",
    "        on_police = lambda x,y: x == y\n",
    "        \n",
    "        grid = np.arange(self.nS).reshape((18,18))\n",
    "        \n",
    "        it = np.nditer(grid, flags=['multi_index'])\n",
    "        \n",
    "        while not it.finished:\n",
    "            \n",
    "            s = it.iterindex\n",
    "            \n",
    "            x,y = it.multi_index\n",
    "            \n",
    "            self.P[s] = {a : [[]] for a in range(self.nA)}\n",
    "            \n",
    "            if on_police(x,y):\n",
    "                reward = -50\n",
    "\n",
    "                for action in range(4):\n",
    "                    self.P[s][action] = [([1.0], [self.init_state], [reward])]\n",
    "            \n",
    "            else:\n",
    "                police_moves = self.get_police_next_moves(x,y)\n",
    "                \n",
    "                agent_next_states = self.get_next_states(x, ACTIONS)\n",
    "                \n",
    "                police_next_states = self.get_next_states(y,police_moves)\n",
    "                \n",
    "                prob = 1/len(police_next_states)\n",
    "\n",
    "                if on_bank(x):\n",
    "                    reward = 10\n",
    "                else:\n",
    "                    reward = 0\n",
    "            \n",
    "            \n",
    "                for action_a, ns in agent_next_states.items():\n",
    "                    next_state_list = []\n",
    "                    prob_list = []\n",
    "                    reward_list = []\n",
    "\n",
    "                    for action_p, ns_p in police_next_states.items():\n",
    "                        \n",
    "                        ns_index = self.state_index[ns][ns_p]\n",
    "\n",
    "                        next_state_list.append(ns_index)\n",
    "                        prob_list.append(prob)\n",
    "                        reward_list.append(reward)\n",
    "\n",
    "                    self.P[s][action_a] = [(prob_list, next_state_list, reward_list)]\n",
    "\n",
    "            it.iternext()\n",
    "            \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = BankEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.create_transition_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def value_iteration(env, theta=0.0001, discount_factor=0.6):\n",
    "    \"\"\"\n",
    "    Value Iteration Algorithm.\n",
    "    \n",
    "    Args:\n",
    "        env:env.P represents the transition probabilities of the environment.\n",
    "            env.P[s][a] is a list of transition tuples (prob, next_state, reward, done).\n",
    "            env.nS is a number of states in the environment. \n",
    "            env.nA is a number of actions in the environment.\n",
    "        theta: We stop evaluation once our value function change is less than theta for all states.\n",
    "        discount_factor: Gamma discount factor.\n",
    "        \n",
    "    Returns:\n",
    "        A tuple (policy, V) of the optimal policy and the optimal value function.\n",
    "    \"\"\"\n",
    "    \n",
    "    def isListEmpty(inList):\n",
    "        if isinstance(inList, list): # Is a list\n",
    "            return all( map(isListEmpty, inList) )\n",
    "        return False # Not a list\n",
    "    \n",
    "    \n",
    "    def one_step_lookahead(state, V):\n",
    "        \"\"\"\n",
    "        Helper function to calculate the value for all action in a given state.\n",
    "        \n",
    "        Args:\n",
    "            state: The state to consider (int)\n",
    "            V: The value to use as an estimator, Vector of length env.nS\n",
    "        \n",
    "        Returns:\n",
    "            A vector of length env.nA containing the expected value of each action.\n",
    "        \"\"\"\n",
    "        \n",
    "        A = np.zeros(env.nA)\n",
    "        for a in range(env.nA):\n",
    "            \n",
    "            if len(env.P[state][a][0]) != 0:\n",
    "\n",
    "                for prob, next_state, reward in env.P[state][a]:\n",
    "\n",
    "                    for i in range(len(prob)):\n",
    "                        A[a] += prob[i] * (reward[i] + discount_factor * V[next_state[i]])\n",
    "\n",
    "               \n",
    "        return A\n",
    "    \n",
    "    V = np.zeros(env.nS)\n",
    "    maxIter = 50\n",
    "    i = 0\n",
    "    while True:\n",
    "        # Stopping condition\n",
    "        delta = 0\n",
    "        # Update each state...\n",
    "        for s in range(env.nS):\n",
    "            \n",
    "            # Do a one-step lookahead to find the best action\n",
    "            A = one_step_lookahead(s, V)\n",
    "            best_action_value = np.max(A)\n",
    "            # Calculate delta across all states seen so far\n",
    "            delta = max(delta, np.abs(best_action_value - V[s]))\n",
    "            # Update the value function. Ref: Sutton book eq. 4.10. \n",
    "            V[s] = best_action_value        \n",
    "        # Check if we can stop \n",
    "        \n",
    "        i +=1\n",
    "        #print(\"Delta-Value \", delta)\n",
    "        if delta < theta:\n",
    "            break\n",
    "        if i >=maxIter:\n",
    "            break\n",
    "        \n",
    "    \n",
    "    # Create a deterministic policy using the optimal value function\n",
    "    policy = np.zeros([env.nS, env.nA])\n",
    "    for s in range(env.nS):\n",
    "        # One step lookahead to find the best action for this state\n",
    "        A = one_step_lookahead(s, V)\n",
    "        best_action = np.argmax(A)\n",
    "        # Always take the best action\n",
    "        policy[s, best_action] = 1.0\n",
    "    \n",
    "    return policy, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "policy, v = value_iteration(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pol = np.reshape(np.argmax(policy, axis=1), (18,18))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "324"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 4, 4, 2, 2, 0],\n",
       "       [1, 1, 1, 2, 2, 1],\n",
       "       [1, 4, 4, 2, 2, 0]], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pol[:,13].reshape((3,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Solve the problem, and display the value function (evaluated at the initial state) as a function\n",
    "# of lambda. Illustrate an optimal policy for different values of lambda - comment on the behaviour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22.61202818579331"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "values = []\n",
    "\n",
    "for discount in np.arange(0,1,0.01):\n",
    "    policy,v = value_iteration(env, theta=0.0001, discount_factor=discount)\n",
    "    values.append(v[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "discounts = np.arange(0,1,0.01)\n",
    "\n",
    "plt.plot(discounts, values)\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "class Simulation:\n",
    "    \n",
    "    def __init__(self, policy):\n",
    "        self.grid = self.initialize_grid()\n",
    "        self.policy = policy\n",
    "        self.s = {'A':[0,0], 'P':[1,2]}\n",
    "    \n",
    "    \n",
    "    def update_state(self, action):\n",
    "                \n",
    "        if action == UP:\n",
    "            self.s['A'][0] -= 1\n",
    "        elif action == DOWN:\n",
    "            self.s['A'][0] += 1\n",
    "        elif action == RIGHT:\n",
    "            self.s['A'][1] += 1\n",
    "        elif action == LEFT:\n",
    "            self.s['A'][1] -= 1\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        action = self.get_police_action()\n",
    "        \n",
    "        while not self.valid_move(self.s['P'], action):\n",
    "            action = self.get_police_action()\n",
    "        \n",
    "        if action == UP:\n",
    "            self.s['P'][0] -= 1\n",
    "        elif action == DOWN:\n",
    "            self.s['P'][0] += 1\n",
    "        elif action == RIGHT:\n",
    "            self.s['P'][1] += 1\n",
    "        elif action == LEFT:\n",
    "            self.s['P'][1] -= 1\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    def initialize_grid(self):\n",
    "        \n",
    "        grid = np.zeros((3,6), dtype = object)\n",
    "        grid[:] = '-'\n",
    "        grid[0,0] = 'A'\n",
    "        grid[1,2] = 'P'\n",
    "        return grid\n",
    "    def update_grid(self):\n",
    "        \n",
    "        ia, ja = self.s['A']\n",
    "        ib, jb = self.s['P']\n",
    "        self.grid = np.zeros((3, 6), dtype = object)\n",
    "        self.grid[:] = '-'\n",
    "        self.grid[ia, ja] = 'A'\n",
    "        self.grid[ib, jb] = 'P'\n",
    "        \n",
    "    def get_police_action(self):\n",
    "        \n",
    "        state_index = np.arange(18).reshape((3,6))\n",
    "\n",
    "        row_agent, col_agent = self.s['A']\n",
    "        row_police, col_police = self.s['P']\n",
    "        \n",
    "        print(\"Agent state\", self.s['A'])\n",
    "        print(\"Police state\", self.s['P'])\n",
    "        \n",
    "        police_state = state_index[row_police][col_police]\n",
    "        agent_state = state_index[row_agent][col_agent]\n",
    "        \n",
    "        if row_agent == row_police:\n",
    "            \n",
    "            if agent_state > police_state:\n",
    "                # UP, DOWN, RIGHT\n",
    "                Moves = [UP, DOWN, RIGHT]\n",
    "            else:\n",
    "                # UP, DOWN, LEFT\n",
    "                Moves = [UP, DOWN, LEFT]\n",
    "\n",
    "        elif col_agent == col_police:\n",
    "            \n",
    "            if agent_state < police_state:   \n",
    "                Moves = [UP, RIGHT, LEFT]\n",
    "            else:\n",
    "                Moves = [DOWN, RIGHT, LEFT]\n",
    "                \n",
    "        else:\n",
    "            # If agent above and right\n",
    "            if agent_state < police_state and col_agent > col_police:\n",
    "                Moves = [UP, RIGHT]\n",
    "                \n",
    "            # If agent above and left\n",
    "            elif agent_state < police_state and col_agent < col_police:\n",
    "                Moves = [UP, LEFT]\n",
    "            \n",
    "            # If agent down and right\n",
    "            elif agent_state > police_state and col_agent > col_police:\n",
    "                Moves = [DOWN, RIGHT]\n",
    "            \n",
    "            # If agent down and left\n",
    "            elif agent_state > police_state and col_agent < col_police:\n",
    "                Moves = [DOWN, LEFT]\n",
    "        str_moves = [STR_ACTIONS[move] for move in Moves]\n",
    "        print(str_moves)\n",
    "        random_action = random.choice(Moves)\n",
    "        print(STR_ACTIONS[random_action])\n",
    "        print('\\n')\n",
    "        return random_action\n",
    "        \n",
    "    def valid_move(self,state, action):\n",
    "        \n",
    "        if action == UP and state[0] == 0:\n",
    "            return False\n",
    "        elif action == DOWN and state[0]:\n",
    "            return False\n",
    "        elif a == LEFT and state[1] == 0: # Western Maze Wall\n",
    "            return False\n",
    "        elif a == RIGHT and state[1] == 5: # Eastern Maze Wall\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "    \n",
    "    def get_action_from_policy(self, state):\n",
    "        \n",
    "        index = np.arange(18).reshape((3,6))\n",
    "        state_index = np.arange(18*18).reshape((18,18))\n",
    "        \n",
    "        col_agent, line_agent = state['A']\n",
    "        col_police, line_police = state['P']\n",
    "        \n",
    "        police_index = index[col_police][line_police]\n",
    "        agent_index = index[col_agent][line_agent]\n",
    "        \n",
    "        state_i = state_index[agent_index][police_index]\n",
    "        \n",
    "        action = self.policy[state_i]\n",
    "        \n",
    "        return action\n",
    "        \n",
    "    def run_simulation(self, T):\n",
    "        \n",
    "        self.s = {'A': [0,0], 'P': [1,2]}\n",
    "        print('Start grid:')\n",
    "        print(self.grid)\n",
    "        print('\\n')\n",
    "        for i in range(T):\n",
    "            \n",
    "            action = self.get_action_from_policy(self.s)\n",
    "            self.update_state(action)\n",
    "            self.update_grid()\n",
    "            print(self.grid)\n",
    "            print('\\n')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    policy = np.argmax(policy,axis=1)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = Simulation(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start grid:\n",
      "[['A' '-' '-' '-' '-' '-']\n",
      " ['-' '-' 'P' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-']]\n",
      "\n",
      "\n",
      "Agent state [0, 0]\n",
      "Police state [1, 2]\n",
      "['UP', 'LEFT']\n",
      "UP\n",
      "\n",
      "\n",
      "[['A' '-' 'P' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-']]\n",
      "\n",
      "\n",
      "Agent state [0, 0]\n",
      "Police state [0, 2]\n",
      "['UP', 'DOWN', 'LEFT']\n",
      "LEFT\n",
      "\n",
      "\n",
      "[['A' 'P' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-']]\n",
      "\n",
      "\n",
      "Agent state [1, 0]\n",
      "Police state [0, 1]\n",
      "['DOWN', 'LEFT']\n",
      "LEFT\n",
      "\n",
      "\n",
      "[['P' '-' '-' '-' '-' '-']\n",
      " ['A' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-']]\n",
      "\n",
      "\n",
      "Agent state [2, 0]\n",
      "Police state [0, 0]\n",
      "['DOWN', 'RIGHT', 'LEFT']\n",
      "LEFT\n",
      "\n",
      "\n",
      "[['-' '-' '-' '-' '-' 'P']\n",
      " ['-' '-' '-' '-' '-' '-']\n",
      " ['A' '-' '-' '-' '-' '-']]\n",
      "\n",
      "\n",
      "Agent state [2, 0]\n",
      "Police state [0, -1]\n",
      "['DOWN', 'RIGHT']\n",
      "DOWN\n",
      "\n",
      "\n",
      "[['-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' 'P']\n",
      " ['A' '-' '-' '-' '-' '-']]\n",
      "\n",
      "\n",
      "Agent state [2, 0]\n",
      "Police state [1, -1]\n",
      "['DOWN', 'RIGHT']\n",
      "DOWN\n",
      "\n",
      "\n",
      "Agent state [2, 0]\n",
      "Police state [1, -1]\n",
      "['DOWN', 'RIGHT']\n",
      "RIGHT\n",
      "\n",
      "\n",
      "[['-' '-' '-' '-' '-' '-']\n",
      " ['P' '-' '-' '-' '-' '-']\n",
      " ['A' '-' '-' '-' '-' '-']]\n",
      "\n",
      "\n",
      "Agent state [2, 1]\n",
      "Police state [1, 0]\n",
      "['DOWN', 'RIGHT']\n",
      "RIGHT\n",
      "\n",
      "\n",
      "[['-' '-' '-' '-' '-' '-']\n",
      " ['-' 'P' '-' '-' '-' '-']\n",
      " ['-' 'A' '-' '-' '-' '-']]\n",
      "\n",
      "\n",
      "Agent state [2, 0]\n",
      "Police state [1, 1]\n",
      "['DOWN', 'LEFT']\n",
      "DOWN\n",
      "\n",
      "\n",
      "Agent state [2, 0]\n",
      "Police state [1, 1]\n",
      "['DOWN', 'LEFT']\n",
      "DOWN\n",
      "\n",
      "\n",
      "Agent state [2, 0]\n",
      "Police state [1, 1]\n",
      "['DOWN', 'LEFT']\n",
      "DOWN\n",
      "\n",
      "\n",
      "Agent state [2, 0]\n",
      "Police state [1, 1]\n",
      "['DOWN', 'LEFT']\n",
      "LEFT\n",
      "\n",
      "\n",
      "[['-' '-' '-' '-' '-' '-']\n",
      " ['P' '-' '-' '-' '-' '-']\n",
      " ['A' '-' '-' '-' '-' '-']]\n",
      "\n",
      "\n",
      "Agent state [2, 1]\n",
      "Police state [1, 0]\n",
      "['DOWN', 'RIGHT']\n",
      "DOWN\n",
      "\n",
      "\n",
      "Agent state [2, 1]\n",
      "Police state [1, 0]\n",
      "['DOWN', 'RIGHT']\n",
      "RIGHT\n",
      "\n",
      "\n",
      "[['-' '-' '-' '-' '-' '-']\n",
      " ['-' 'P' '-' '-' '-' '-']\n",
      " ['-' 'A' '-' '-' '-' '-']]\n",
      "\n",
      "\n",
      "Agent state [2, 0]\n",
      "Police state [1, 1]\n",
      "['DOWN', 'LEFT']\n",
      "DOWN\n",
      "\n",
      "\n",
      "Agent state [2, 0]\n",
      "Police state [1, 1]\n",
      "['DOWN', 'LEFT']\n",
      "DOWN\n",
      "\n",
      "\n",
      "Agent state [2, 0]\n",
      "Police state [1, 1]\n",
      "['DOWN', 'LEFT']\n",
      "LEFT\n",
      "\n",
      "\n",
      "[['-' '-' '-' '-' '-' '-']\n",
      " ['P' '-' '-' '-' '-' '-']\n",
      " ['A' '-' '-' '-' '-' '-']]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a.run_simulation(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
